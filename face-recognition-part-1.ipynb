{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Face recognition with Tensorflow Object Detection API (LFW dataset)","metadata":{"_uuid":"c145257f431eea690a59385a2f1cde82f7409924"}},{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#Visiulazation\nimport matplotlib.pyplot as plt\n#image processing\nimport cv2\n#extracting zippped file\nimport tarfile\n#systems\nimport os\nprint(os.listdir(\"../input\"))","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-12-28T08:49:05.887343Z","iopub.execute_input":"2022-12-28T08:49:05.887644Z","iopub.status.idle":"2022-12-28T08:49:05.893737Z","shell.execute_reply.started":"2022-12-28T08:49:05.887595Z","shell.execute_reply":"2022-12-28T08:49:05.892418Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"['haarcascades', 'lfwpeople']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This is the complete code for part 1 of [my Medium article](https://medium.com/@saidakbarp/real-time-face-recognition-tflite-3fb818ac039a). The complete notebook for the part 2 of my Medium article is [here](https://www.kaggle.com/saidakbarp/face-recognition-part-2/notebook).\n\nFace recognition relies on the dataset that has been annotated with boxes. Manually annotating faces in each images can be time consuming and for large scale training, manual annotation is impractical. For this reason we will use available face annotation tools to annotate each image with boxes. Afterwards, we can move to Object detection training part.","metadata":{"_uuid":"7c6f7fbef29d879ec925a743c0c6fcaeac50865c"}},{"cell_type":"code","source":"#example\nimgg=\"/kaggle/input/photos/ben.jpg\"\nceleb=cv2.imread(imgg)","metadata":{"_uuid":"12983332defabe2f3e6ba4b312fb717649e4472b","execution":{"iopub.status.busy":"2022-12-28T08:49:05.900579Z","iopub.execute_input":"2022-12-28T08:49:05.900972Z","iopub.status.idle":"2022-12-28T08:49:05.905855Z","shell.execute_reply.started":"2022-12-28T08:49:05.900928Z","shell.execute_reply":"2022-12-28T08:49:05.904838Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def show_image(image):\n    plt.figure(figsize=(8,5))\n    #Before showing image, bgr color order transformed to rgb order\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    plt.xticks([])\n    plt.yticks([])\n    plt.show()\n    ","metadata":{"_uuid":"b395e885e4c4c963557d11f6ba67e9612e189225","execution":{"iopub.status.busy":"2022-12-28T08:49:05.912059Z","iopub.execute_input":"2022-12-28T08:49:05.912390Z","iopub.status.idle":"2022-12-28T08:49:05.918618Z","shell.execute_reply.started":"2022-12-28T08:49:05.912335Z","shell.execute_reply":"2022-12-28T08:49:05.917334Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"show_image(celeb)","metadata":{"_uuid":"8fd0dcafc3951e089ca1c1b41e941ed0f8e1aaba","execution":{"iopub.status.busy":"2022-12-28T08:49:05.920151Z","iopub.execute_input":"2022-12-28T08:49:05.920487Z","iopub.status.idle":"2022-12-28T08:49:06.024557Z","shell.execute_reply.started":"2022-12-28T08:49:05.920426Z","shell.execute_reply":"2022-12-28T08:49:05.954796Z"},"trusted":true},"execution_count":27,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-e77befdd13d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mceleb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-26-1f4d5845c316>\u001b[0m in \u001b[0;36mshow_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#Before showing image, bgr color order transformed to rgb order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.0.0) /io/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"],"ename":"error","evalue":"OpenCV(4.0.0) /io/opencv/modules/imgproc/src/color.cpp:181: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x360 with 0 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"# Our face detection function that uses haarcascade from OpenCV\ndef face_detection(img):\n    face_cascade = cv2.CascadeClassifier('/kaggle/input/haarcascades/haarcascade_frontalface_alt.xml')\n    \n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(gray)\n    print('Number of faces detected:', len(faces))\n        \n    for (x,y,w,h) in faces:\n        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n        #img = img[y:y+h, x:x+w] # for cropping\n    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return cv_rgb","metadata":{"_uuid":"7e9517349c29c7806b3ff312c1af45aad871b269","execution":{"iopub.status.busy":"2022-12-28T08:49:05.955824Z","iopub.status.idle":"2022-12-28T08:49:05.956422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#imgg2=cv2.imread(\"/kaggle/input/photos/ben.jpg\")\na=face_detection(celeb)\nplt.imshow(a)\nplt.show() \n\n# as shown below, the library is not detecting this particular face angle of Ben Afflek","metadata":{"_uuid":"b9018f1f132efe13cc497df1bf1bb1beaaea8469","execution":{"iopub.status.busy":"2022-12-28T08:49:05.957462Z","iopub.status.idle":"2022-12-28T08:49:05.958098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,18))\nimg=cv2.imread(\"../input/photos/people.jpg\")\nc=face_detection(img)\nplt.imshow(c)\nplt.show()","metadata":{"_uuid":"c76defb5172473950560d19e01ba89883e8cd2ec","execution":{"iopub.status.busy":"2022-12-28T08:49:05.958930Z","iopub.status.idle":"2022-12-28T08:49:05.959576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Haarcascade has a con: it can not detect non-frontal face images and also boxes sometimes do not include full face, clipping chins or forehead. Let us try a better model: openCV DNN","metadata":{"_uuid":"424c50a14001ab11586a59c93b0b46caf3b4a9c6"}},{"cell_type":"code","source":"# using openCV DNN\n# load the model\n#modelFile = \"../input/opencv-dnn/opencv_face_detector_uint8.pb\" \n#configFile = \"../input/opencv-dnn/opencv_face_detector.pbtxt\"\n#net = cv2.dnn.readNetFromTensorflow(modelFile, configFile)\n\nmodelFile =\"../input/opencvdnnfp16/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\nconfigFile = \"../input/opencvdnnfp16/deploy.prototxt\"\nnet = cv2.dnn.readNetFromCaffe(configFile, modelFile)","metadata":{"_uuid":"86ded388129c9caeb396d6439f5b10f0cdcce6f3","execution":{"iopub.status.busy":"2022-12-28T08:49:05.960521Z","iopub.status.idle":"2022-12-28T08:49:05.961481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#function to extract box dimensions\ndef face_dnn(img, coord=False):\n    blob = cv2.dnn.blobFromImage(img, 1, (224,224), [104, 117, 123], False, False) #\n    # params: source, scale=1, size=300,300, mean RGB values (r,g,b), rgb swapping=false, crop = false\n    conf_threshold=0.8 # confidence at least 60%\n    frameWidth=img.shape[1] # get image width\n    frameHeight=img.shape[0] # get image height\n    max_confidence=0\n    net.setInput(blob)\n    detections = net.forward()\n    detection_index=0\n    bboxes = []\n    \n    for i in range(detections.shape[2]):\n        confidence = detections[0, 0, i, 2]\n        if confidence > conf_threshold:\n            \n            if max_confidence < confidence: # only show maximum confidence face\n                max_confidence = confidence\n                detection_index = i\n    i=detection_index        \n    x1 = int(detections[0, 0, i, 3] * frameWidth)\n    y1 = int(detections[0, 0, i, 4] * frameHeight)\n    x2 = int(detections[0, 0, i, 5] * frameWidth)\n    y2 = int(detections[0, 0, i, 6] * frameHeight)\n    cv2.rectangle(img,(x1,y1),(x2,y2),(255,255,0),2)\n    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    if coord==True:\n        return x1, y1, x2, y2\n    return cv_rgb","metadata":{"_uuid":"ff055854fdc750b13176abc4443a626774176ec6","execution":{"iopub.status.busy":"2022-12-28T08:49:05.972449Z","iopub.status.idle":"2022-12-28T08:49:05.972934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#gray=cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n# multiple faces needs increasing the size of image as well as multiple detections\ndef nfaces_dnn(img):\n    blob = cv2.dnn.blobFromImage(img, 1.2, (1200,1200), [104, 117, 123], False, False) #\n    # params: source, scale=1, size=300,300, mean RGB values (r,g,b), rgb swapping=false, crop = false\n    conf_threshold=0.6 # confidence at least 60%\n    frameWidth=img.shape[1] # get image width\n    frameHeight=img.shape[0] # get image height\n    net.setInput(blob)\n    detections = net.forward()\n  \n    bboxes = []\n    \n    for i in range(detections.shape[2]):\n        confidence = detections[0, 0, i, 2]\n        if confidence > conf_threshold:\n                  \n            x1 = int(detections[0, 0, i, 3] * frameWidth)\n            y1 = int(detections[0, 0, i, 4] * frameHeight)\n            x2 = int(detections[0, 0, i, 5] * frameWidth)\n            y2 = int(detections[0, 0, i, 6] * frameHeight)\n            cv2.rectangle(img,(x1,y1),(x2,y2),(255,255,0),2)\n    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return cv_rgb","metadata":{"_uuid":"f244605b1f3a92d48755c8ff87b1b87c3a59c0f0","execution":{"iopub.status.busy":"2022-12-28T08:49:06.147994Z","iopub.execute_input":"2022-12-28T08:49:06.148574Z","iopub.status.idle":"2022-12-28T08:49:06.159002Z","shell.execute_reply.started":"2022-12-28T08:49:06.148515Z","shell.execute_reply":"2022-12-28T08:49:06.157845Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"a=face_dnn(celeb)\nplt.imshow(a)\nplt.show() ","metadata":{"_uuid":"769167b85005e67201d53cb5a2f63ca87f40d1a5","execution":{"iopub.status.busy":"2022-12-28T08:49:06.160261Z","iopub.execute_input":"2022-12-28T08:49:06.160597Z","iopub.status.idle":"2022-12-28T08:49:06.306217Z","shell.execute_reply.started":"2022-12-28T08:49:06.160526Z","shell.execute_reply":"2022-12-28T08:49:06.185786Z"},"trusted":true},"execution_count":29,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-7416213db9d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mface_dnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mceleb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'face_dnn' is not defined"],"ename":"NameError","evalue":"name 'face_dnn' is not defined","output_type":"error"}]},{"cell_type":"code","source":"img=cv2.imread(\"../input/photos/people.jpg\")\nc=nfaces_dnn(img)\nplt.figure(figsize=(15,18))\nplt.imshow(c)\nplt.show()","metadata":{"_uuid":"5b4c0298e48b104d3b57caa34c6a5139d9730fe0","execution":{"iopub.status.busy":"2022-12-28T08:49:06.186567Z","iopub.status.idle":"2022-12-28T08:49:06.187194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As output shows, although 'multiple faces' function does not register some faces from a group of people, DNN is able to detect non-frontal faces without clipping chin.","metadata":{"_uuid":"db3cf1a12c55c40dc904a445f9c107afae13d33c"}},{"cell_type":"markdown","source":"Our main purpose is to use LFW people images that have at least 20 examples ( later, obtain 100 images of top 100 celebrities from Google image search), then crop their faces (single face photos). Afterwards, we train our mobilenetV2 coco model to recognize those celebrities. Final goal is to identify celebrities from their photos.","metadata":{"_uuid":"fa578d0a48bdbeb5b8f8dbd9536b33ddf17fafc8"}},{"cell_type":"markdown","source":"## Now we extract LFW for face detection","metadata":{"_uuid":"c9c4fa59090210b1023ab7ab92e178fa45672b84"}},{"cell_type":"markdown","source":"Each picture is centered on a single face. The typical task is called Face Verification: given a pair of two pictures, a binary classifier must predict whether the two images are from the same person.\n\nAn alternative task, Face Recognition or Face Identification is: given the picture of the face of an unknown person, identify the name of the person by referring to a gallery of previously seen pictures of identified persons.\n\nBoth Face Verification and Face Recognition are tasks that are typically performed on the output of a model trained to perform Face Detection. The most popular model for Face Detection is called Viola-Jones and is implemented in the OpenCV library. The LFW faces were extracted by this face detector from various online websites.","metadata":{"_uuid":"0b633fad9312775ef5c8229f158342d7569d6ba1"}},{"cell_type":"code","source":"#os.listdir('../input/lfwpeople/')\nfname='../input/lfwpeople/lfwfunneled.tgz'","metadata":{"_uuid":"bfc8f80a12b3a38b84659236865ef6b582308fbd","execution":{"iopub.status.busy":"2022-12-28T08:49:06.187964Z","iopub.status.idle":"2022-12-28T08:49:06.188560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def jpg_files(members): #only extract jpg files\n    for tarinfo in members:\n        if os.path.splitext(tarinfo.name)[1] == \".jpg\":\n            yield tarinfo\ndef untar(fname,path=\"LFW\"): #untarring the archive\n    tar = tarfile.open(fname)\n    tar.extractall(path,members=jpg_files(tar))\n    tar.close()\n    if path is \"\":\n        print(\"File Extracted in Current Directory\")\n    else:\n        print(\"File Extracted in to\",  path)","metadata":{"_uuid":"1e10bafc27840d3744dddfecb1ed6856c7d7888e","execution":{"iopub.status.busy":"2022-12-28T08:49:06.189307Z","iopub.status.idle":"2022-12-28T08:49:06.189870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"untar(fname,\"LFW\")","metadata":{"_uuid":"c4a45432dd47e724a4cb86043658fe74d23014f0","execution":{"iopub.status.busy":"2022-12-28T08:49:06.190593Z","iopub.status.idle":"2022-12-28T08:49:06.191150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(os.listdir('../working/LFW/lfw_funneled/')) # total number of folders (people)","metadata":{"_uuid":"7436a1ad5ca7b4bba079d81712ea12bca2d0e0d6","execution":{"iopub.status.busy":"2022-12-28T08:49:06.191875Z","iopub.status.idle":"2022-12-28T08:49:06.192483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total number of images\ntotal = sum([len(files) for r, d, files in os.walk('../working/LFW/lfw_funneled/')])\nprint(total)","metadata":{"_uuid":"d17df96e44b0d2df41dfb11dbfe139598b605c62","execution":{"iopub.status.busy":"2022-12-28T08:49:06.193211Z","iopub.status.idle":"2022-12-28T08:49:06.193943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0 \nimglist=[]\n# \nfor r, d, files in os.walk('../working/LFW/lfw_funneled/'):   \n    if len(files)>=20: \n        imglist.append(r)\n        #print(count, r)\n        count+=1 # counts how many folders have with at least 20 images\nprint(count)","metadata":{"_uuid":"b2661b21895fd41a209e9bd0ebc9cc346e5713b5","execution":{"iopub.status.busy":"2022-12-28T08:49:06.195171Z","iopub.status.idle":"2022-12-28T08:49:06.196061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#os.listdir(imglist[0])\n# pick one random photo\na=np.random.randint(0,20)\nb=np.random.randint(0,62)\nimglist[b]","metadata":{"_uuid":"252eda3e435f4762f0f1ebcb0f13e73723daf0e6","execution":{"iopub.status.busy":"2022-12-28T08:49:06.196861Z","iopub.status.idle":"2022-12-28T08:49:06.197663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show the random photo with face detected\nimg=imglist[b]+'/'+os.listdir(imglist[b])[a]\nimg=cv2.imread(img)\nc=face_dnn(img)\nplt.imshow(c)\nplt.show()","metadata":{"_uuid":"fd52c8a1fd8b4e5564b7c86532df61e849cbb543","execution":{"iopub.status.busy":"2022-12-28T08:49:06.198381Z","iopub.status.idle":"2022-12-28T08:49:06.198863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a list of people (62) with at least 20 images. Now, we will use this annotation of faces, and train MobileNet NN model to recognize those people.","metadata":{"_uuid":"d912b24e81163c53c6e01976634a72711474892d"}},{"cell_type":"markdown","source":"## Creating training and test sets","metadata":{"_uuid":"0b798334765de49f1ed1f3581f9e1461b9b9df65"}},{"cell_type":"code","source":"#remove unused folders\nimport shutil\npathd='../working/LFW/lfw_funneled/'\n#shutil.rmtree(os.path.realpath('LFW'))\nfor dirs in os.listdir(pathd):\n    if not (pathd+dirs) in imglist:\n        shutil.rmtree(os.path.realpath(pathd+dirs))","metadata":{"_uuid":"0fa5af11a548f24f4ae0b6838bd8eda53e4473de","execution":{"iopub.status.busy":"2022-12-28T08:49:06.199833Z","iopub.status.idle":"2022-12-28T08:49:06.200585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dirs=os.listdir(pathd)\ndirs.sort()","metadata":{"_uuid":"94780f9233102a93bb7bbba73a46d3ee8038f325","execution":{"iopub.status.busy":"2022-12-28T08:49:06.201326Z","iopub.status.idle":"2022-12-28T08:49:06.201857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#example of box coordinates\n#a=np.random.randint(0,20)\nb=np.random.randint(0,62)\nfor img in os.listdir(pathd+dirs[b])[:5]:\n    #print(pathd+dirs[0]+'/'+img)\n    print(dirs[b])\n    img=cv2.imread(pathd+dirs[b]+'/'+img)\n    x1, y1, x2, y2=face_dnn(img, True)\n    #print coordinates of the detected face\n    print(x1, y1, x2, y2)\n    plt.imshow(img)\n    plt.show()","metadata":{"_uuid":"52d9a6bd8a88163bcd1cc66add76f0145668fa65","execution":{"iopub.status.busy":"2022-12-28T08:49:06.202665Z","iopub.status.idle":"2022-12-28T08:49:06.203225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('../working/LFW/')\npathd\n#(os.listdir(pathd))","metadata":{"_uuid":"a0abc746b3dc6fc47117f08a28fed9fa65c86431","execution":{"iopub.status.busy":"2022-12-28T08:49:06.204030Z","iopub.status.idle":"2022-12-28T08:49:06.204711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # creating test and train set\nfrom numpy import random\ndatadir='../working/LFW/'\ntrain=datadir+'train/'\ntest=datadir+'test/'\n\nif not os.path.exists(train):\n    os.mkdir(train)\nif not os.path.exists(test):\n    os.mkdir(test)\n    \nfor dirs in os.listdir(pathd):\n    filenames = os.listdir(pathd+dirs)\n    filenames.sort()  # make sure that the filenames have a fixed order before shuffling\n    random.seed(402)\n    random.shuffle(filenames) # shuffles the ordering of filenames (deterministic given the chosen seed)\n    split = int(0.85 * len(filenames))\n    train_filenames = filenames[:split] # splitting filenames into two parts\n    test_filenames = filenames[split:]\n    for img in train_filenames:\n        full_file_name = os.path.join(pathd+dirs, img)\n        cur_dir=os.path.join(train+dirs)\n        #print(cur_dir)\n        if not os.path.exists(cur_dir): # create this current person's folder for training\n            os.mkdir(cur_dir)\n        shutil.copy(full_file_name, cur_dir)\n    for img in test_filenames:\n        full_file_name = os.path.join(pathd+dirs, img)\n        cur_dir=os.path.join(test+dirs)\n        if not os.path.exists(cur_dir): # create this current person's folder for testing\n            os.mkdir(cur_dir)\n        shutil.copy(full_file_name, cur_dir)\n        #a=full_file_name+' '+test+dirs\nshutil.rmtree('../working/LFW/lfw_funneled/')","metadata":{"_uuid":"37208ea1dd47032adf7a3ff5615a8f65bf1d1c6f","execution":{"iopub.status.busy":"2022-12-28T08:49:06.205558Z","iopub.status.idle":"2022-12-28T08:49:06.206050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total number of images left\ntotal = sum([len(files) for r, d, files in os.walk(datadir)])\nprint(total)","metadata":{"_uuid":"aa66ba219c5d15bb5110479d11b10d0c2ea19049","execution":{"iopub.status.busy":"2022-12-28T08:49:06.206749Z","iopub.status.idle":"2022-12-28T08:49:06.207597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labeldir=\"Labels/\" # labels dir\nwdir=\"../working/LFW/\"\nlab=wdir+labeldir\nif not os.path.exists(lab):\n    os.mkdir(lab)","metadata":{"_uuid":"f6ef8a35624b61d11138e4f128b873bf5bda2413","execution":{"iopub.status.busy":"2022-12-28T08:49:06.214731Z","iopub.status.idle":"2022-12-28T08:49:06.215364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for creating box labels as txt file\ndef label_txt(pathdr, lab_dir):\n    for fol in os.listdir(pathdr):\n        tfile = open(lab_dir+fol+\".txt\",\"w+\")\n        for img in os.listdir(pathdr+fol):\n            pathimg=os.path.join(pathdr+fol, img)\n            #print(pathimg)\n            pic=cv2.imread(pathimg)\n            x1, y1, x2, y2=face_dnn(pic, True) # face detection and then saving into txt file       \n            tfile.write(img+' '+str(x1)+' '+str(x2)+' '+str(y1)+' '+str(y2)+'\\n')          \n        tfile.close()\n    print('Saved')","metadata":{"_uuid":"805ec0f78e02b90497339e39239e09be8c631f8f","execution":{"iopub.status.busy":"2022-12-28T08:49:06.221731Z","iopub.status.idle":"2022-12-28T08:49:06.222459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lab_dir=lab+'train/'\nos.mkdir(lab_dir)\nlabel_txt(train, lab_dir)\n\nlab_dir=lab+'test/'\nos.mkdir(lab_dir)\nlabel_txt(test, lab_dir)","metadata":{"_uuid":"9d61403d79f8dc5c358570dab213f3fc890cdbe5","execution":{"iopub.status.busy":"2022-12-28T08:49:06.228199Z","iopub.status.idle":"2022-12-28T08:49:06.228758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#let's check if txt files are correct:\nf2 = open(\"../working/LFW/Labels/test/Arnold_Schwarzenegger.txt\",\"r\")\nprint(f2.read())\nf2.close()","metadata":{"_uuid":"b86f87860d85c38bc6156197be2085cf939f26e1","execution":{"iopub.status.busy":"2022-12-28T08:49:06.234531Z","iopub.status.idle":"2022-12-28T08:49:06.235540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking Arnold_Schwarzenegger_0041.jpg\npic=cv2.imread(test+'Arnold_Schwarzenegger/Arnold_Schwarzenegger_0041.jpg')\ncv2.rectangle(pic,(77 ,62),(176,194),(255,255,0),2)\nplt.imshow(pic)\nplt.show()","metadata":{"_uuid":"0f4bfb31b07dc9d8a3aed66c7f24cb70523be108","execution":{"iopub.status.busy":"2022-12-28T08:49:06.240746Z","iopub.status.idle":"2022-12-28T08:49:06.241938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see box annotation is correct! Now, we have to save annotations as tfrecord:","metadata":{"_uuid":"762ab23282a317bbd96c7107e8c9ea0d319dd383"}},{"cell_type":"markdown","source":"# Creating tfrecords with annotated images","metadata":{"_uuid":"9cb6593b281ed0b933a7988799b42ea34894749e"}},{"cell_type":"code","source":"def read_txt(person, photo):  \n    txtfile = labels+person+\".txt\"\n    txtfile_contents = open(txtfile, \"r\")\n    txtlines = txtfile_contents.readlines()\n    txtfile_contents.close()\n    for line in txtlines:\n        if photo in line:\n            txtlines=line\n    return txtlines","metadata":{"_uuid":"7dfd5ae7d62f7e11e299ac7a13483e28181bdd88","execution":{"iopub.status.busy":"2022-12-28T08:49:06.246461Z","iopub.status.idle":"2022-12-28T08:49:06.247182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install object_detection >obj_dtc.txt # installing object detection","metadata":{"_uuid":"93064ab42a5d8eaf17a58ef32135e4cba8004670","execution":{"iopub.status.busy":"2022-12-28T08:49:06.252232Z","iopub.status.idle":"2022-12-28T08:49:06.253142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom object_detection.utils import dataset_util\n\n# flags = tf.app.flags\n# flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n# FLAGS = flags.FLAGS\n# modified from source: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md\ndef create_tf_example(photo, person, iclass, foldr):\n    # one image at a time\n    img_f=os.path.join(foldr+person,photo+\".jpg\")\n    pic = Image.open(img_f)\n    height = pic.height # Image height\n    width = pic.width # Image width\n    filename = str.encode(photo) # Filename of the image. Empty if image is not from file\n    #encoded_image_data = None # Encoded image bytes\n    image_data = tf.gfile.GFile(img_f,'rb').read()\n    \n    image_format = b'jpeg' #None #  or b'png'\n    #declare coordinates\n    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n    xmaxs = [] # List of normalized right x coordinates in bounding box\n             # (1 per box)\n    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n             # (1 per box)\n    classes_text = [] # List of string class name of bounding box (1 per box)\n    classes = [] # List of integer class id of bounding box (1 per box)\n    \n    txtlines = read_txt(person, photo)\n\n    labels = txtlines.split()\n\n    xmins.append(float(labels[1])/width)\n    xmaxs.append(float(labels[2])/width)\n    ymins.append(float(labels[3])/height)\n    ymaxs.append(float(labels[4])/height)\n\n    classes_text.append(str.encode(person))\n    classes.append(iclass) #### iterator is needed\n    #print(xmins, xmaxs, ymins, ymaxs, classes_text, photo, img_f) # for test purposes\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n      'image/height': dataset_util.int64_feature(height),\n      'image/width': dataset_util.int64_feature(width),\n      'image/filename': dataset_util.bytes_feature(filename),\n      'image/source_id': dataset_util.bytes_feature(filename),\n      'image/encoded': dataset_util.bytes_feature(image_data),\n      'image/format': dataset_util.bytes_feature(image_format),\n      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n      'image/object/class/label': dataset_util.int64_list_feature(classes),\n    }))\n    return tf_example\n\n","metadata":{"_uuid":"52983db42070595ada97632d3df69305847f1016","execution":{"iopub.status.busy":"2022-12-28T08:49:06.254000Z","iopub.status.idle":"2022-12-28T08:49:06.254637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n#saving tfrecords\ndef save_tf(folder):\n    tf_file=folder.split('/')[-2] +'.tfrecord'\n    writer = tf.python_io.TFRecordWriter('../working/'+tf_file)\n    \n    labelmap = '../working/'+'object_label.pbtxt' # for model training\n    txtf = open(labelmap, \"w\")\n    \n    labels = '../working/'+'labels.txt' # for android deployment\n    txtl = open(labels, \"w\")\n    \n    for ind, person in enumerate(os.listdir(folder)):\n        iclass=ind+1\n        txtf.write(\"item\\n{\\n  id: %s\\n  name: '%s'\\n}\\n\"%(iclass,person))\n        txtl.write(\"%s\\n\"%person)\n        #print(iclass, person)\n        for photo in os.listdir(folder+person):\n            tf_example = create_tf_example(photo.split('.')[0], person, iclass, folder) #004.jpg, arnold, 1\n            #print('Folder:', pathd+fol, iclass)\n            writer.write(tf_example.SerializeToString())\n    txtf.close()\n    writer.close()","metadata":{"_uuid":"e58bf9013dc22e119667b7279fbcd61ec21d109e","execution":{"iopub.status.busy":"2022-12-28T08:49:06.400618Z","iopub.execute_input":"2022-12-28T08:49:06.401198Z","iopub.status.idle":"2022-12-28T08:49:06.408623Z","shell.execute_reply.started":"2022-12-28T08:49:06.401117Z","shell.execute_reply":"2022-12-28T08:49:06.407799Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"labels='../working/LFW/Labels/train/'\nsave_tf(train)\n\nlabels='../working/LFW/Labels/test/'\nsave_tf(test)","metadata":{"_uuid":"a8c8ea115ca2c9c1403e004f8c0e1254cc24bd80","execution":{"iopub.status.busy":"2022-12-28T08:49:06.409901Z","iopub.execute_input":"2022-12-28T08:49:06.410529Z","iopub.status.idle":"2022-12-28T08:49:06.490904Z","shell.execute_reply.started":"2022-12-28T08:49:06.410462Z","shell.execute_reply":"2022-12-28T08:49:06.431819Z"},"trusted":true},"execution_count":31,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-8d2999be3e1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../working/LFW/Labels/train/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msave_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../working/LFW/Labels/test/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msave_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"],"ename":"NameError","evalue":"name 'train' is not defined","output_type":"error"}]},{"cell_type":"code","source":"os.stat('../working/train.tfrecord').st_size/1024/1024","metadata":{"_uuid":"09388859024fe9131f83892ef1a01c13c6078a5b","execution":{"iopub.status.busy":"2022-12-28T08:49:06.432576Z","iopub.status.idle":"2022-12-28T08:49:06.433366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_tarfile(output_filename, source_dir):\n    with tarfile.open(output_filename, \"w:gz\") as tar:\n        tar.add(source_dir, arcname=os.path.basename(source_dir))\nmake_tarfile('test_images.tar.gz','/kaggle/working/LFW/test')        \nshutil.rmtree('/kaggle/working/LFW/')\n#os.listdir('/kaggle/working/LFW/test')","metadata":{"_uuid":"f86c6e6575ee32d80288d92285737034971713d9","execution":{"iopub.status.busy":"2022-12-28T08:49:06.434784Z","iopub.status.idle":"2022-12-28T08:49:06.435727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir('/kaggle/working/')","metadata":{"_uuid":"a5fb4f24a40dfebec5d110f1c8395c8a92701364","execution":{"iopub.status.busy":"2022-12-28T08:49:06.436630Z","iopub.status.idle":"2022-12-28T08:49:06.437331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for test purposes\n#\npic = Image.open('../working/LFW/test/Jennifer_Capriati/Jennifer_Capriati_0007.jpg')\n# \nheight = pic.height # Image height\n# \nwidth = pic.width # Image width\n# \npic=cv2.imread('../working/LFW/test/Jennifer_Capriati/Jennifer_Capriati_0007.jpg')\n# \ncv2.rectangle(pic,(int(0.304*width),int(0.236*height)),(int(0.692*width),int(0.768*height)),(255,255,0),2)\n# \nplt.imshow(pic)\n# \nplt.show()","metadata":{"_uuid":"7314d592fef766fbb9ca164c5b98a1ad07b0803c","execution":{"iopub.status.busy":"2022-12-28T08:49:06.438206Z","iopub.status.idle":"2022-12-28T08:49:06.439391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our tfrecords and labels are ready for model training. [Part 2](https://www.kaggle.com/saidakbarp/face-recognition-part-2/notebook) shows how to train an existing model and convert it to tflite directly in jupyter notebook.","metadata":{"_uuid":"201b56f389b3110643eb29cf7ccdb11ce885b483"}}]}